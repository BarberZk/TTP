{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-27T18:02:32.563324Z",
     "start_time": "2025-10-27T18:02:27.198106Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:02:32.572025Z",
     "start_time": "2025-10-27T18:02:32.566328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.embeddings import DashScopeEmbeddings"
   ],
   "id": "120591c5727a4152",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:02:32.679690Z",
     "start_time": "2025-10-27T18:02:32.674856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env_path = Path('.') / '.env'\n",
    "load_dotenv(dotenv_path=env_path)\n"
   ],
   "id": "dd10741805d25e30",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:02:33.498105Z",
     "start_time": "2025-10-27T18:02:32.690452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "QWEN_API_KEY = os.environ.get(\"DASHSCOPE_API_KEY\")\n",
    "QWEN_BASE_URL = os.environ.get(\"DASHSCOPE_API_URL\")\n",
    "llm = ChatOpenAI(\n",
    "        model=\"qwen-plus-latest\",\n",
    "        temperature=0.3,\n",
    "        api_key=QWEN_API_KEY,\n",
    "        base_url=QWEN_BASE_URL\n",
    "    )\n",
    "\n",
    "embedding_model = DashScopeEmbeddings(\n",
    "        model=os.environ.get(\"EMBEDDING_MODEL\")\n",
    "        # (它会自动从 DASHSCOPE_API_KEY 环境变量中获取密钥)\n",
    "        # (它会使用正确的、原生的阿里云 embedding 端点)\n",
    "    )"
   ],
   "id": "910b9f41ed39d8b4",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:02:33.506480Z",
     "start_time": "2025-10-27T18:02:33.503704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "DATASET1_PATH = \"./data/input/sft_dataset_4000.json\"\n",
    "DATASET2_PATH = \"./data/input/MITRE-ATTACK_dataset_test.json\"\n",
    "OUTPUT_DATASET_PATH = \"./data/output/sft_dataset_with_qwen_compat_explanations.json\"\n",
    "\n",
    "VECTORDB_PERSIST_DIR = \"./data/database/ttp_chroma_db_qwen_compat/\""
   ],
   "id": "d9f1555445519963",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:02:33.513685Z",
     "start_time": "2025-10-27T18:02:33.511902Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b326258ea2ae3618",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:02:33.522425Z",
     "start_time": "2025-10-27T18:02:33.519880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from langchain_core.documents import Document\n",
    "# df_ttp = pd.read_json(DATASET2_PATH, orient=\"records\")\n",
    "#\n",
    "# required_columns = ['ID', 'name', 'description']\n",
    "# if not all(col in df_ttp.columns for col in required_columns):\n",
    "#     raise ValueError(f\"Dataset 2 JSON file must contain the following keys: {required_columns}\")\n",
    "#\n",
    "#     # 将 DataFrame 转换为 Langchain Document 对象列表\n",
    "# documents = []\n",
    "# for index, row in df_ttp.iterrows():\n",
    "#     doc = Document(\n",
    "#         page_content=row['description'],\n",
    "#         metadata={'id': row['ID'], 'name': row['name']}\n",
    "#     )\n",
    "#     documents.append(doc)\n",
    "# print(f\"✅ Loaded and converted {len(documents)} TTP documents.\")\n"
   ],
   "id": "fd1047b4d84b17fa",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:02:33.567341Z",
     "start_time": "2025-10-27T18:02:33.549090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.documents import Document\n",
    "try:\n",
    "    df_ttp = pd.read_json(DATASET2_PATH, orient=\"records\")\n",
    "\n",
    "    required_columns = ['ID', 'name', 'description']\n",
    "    if not all(col in df_ttp.columns for col in required_columns):\n",
    "        raise ValueError(f\"Dataset 2 JSON file must contain the following keys: {required_columns}\")\n",
    "\n",
    "    documents = []\n",
    "    skipped_rows = 0\n",
    "    # (这是您 naive_rag.ipynb 中的单元格 7 的内容，但增加了修复)\n",
    "    for index, row in df_ttp.iterrows():\n",
    "        description_content = row['description']\n",
    "\n",
    "        # --- !! 关键修复点 (Robust Fix) !! ---\n",
    "        # 我们必须跳过 None/NaN、非字符串类型 *以及* 空字符串\n",
    "        if pd.isna(description_content) or not isinstance(description_content, str) or description_content.strip() == \"\":\n",
    "            print(f\"⚠️ WARNING: Skipping row {index} (ID: {row.get('ID', 'N/A')}) due to missing, empty, or non-string description.\")\n",
    "            skipped_rows += 1\n",
    "            continue # 跳过这一行\n",
    "\n",
    "        # 只有有效的、非空的字符串才会进入这里\n",
    "        doc = Document(\n",
    "            page_content=description_content, # 现在可以安全使用\n",
    "            metadata={'id': row['ID'], 'name': row['name']}\n",
    "        )\n",
    "        documents.append(doc)\n",
    "\n",
    "    print(f\"✅ Loaded and converted {len(documents)} TTP documents.\")\n",
    "    if skipped_rows > 0:\n",
    "        print(f\"⚠️ Skipped {skipped_rows} rows due to invalid/missing descriptions.\")\n",
    "\n",
    "    if not documents:\n",
    "        raise ValueError(\"No valid documents were loaded. Please check your JSON file for 'description' fields.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ ERROR: Knowledge Base file not found at '{DATASET2_PATH}'\")\n",
    "    exit(1)\n",
    "except ValueError as ve:\n",
    "    print(f\"❌ ERROR: {ve}\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"❌ An unexpected error occurred while loading Dataset 2: {e}\")\n",
    "    exit(1)"
   ],
   "id": "309254bad8f7482f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded and converted 508 TTP documents.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:02:33.609486Z",
     "start_time": "2025-10-27T18:02:33.607208Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7436efd52f0e0ebb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:02:33.758710Z",
     "start_time": "2025-10-27T18:02:33.653571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    # (我们现在使用 Chroma.from_documents()，因为 DashScopeEmbeddings 类可以正确处理它)\n",
    "    if os.path.exists(VECTORDB_PERSIST_DIR) and os.listdir(VECTORDB_PERSIST_DIR):\n",
    "        print(f\"    Loading existing vector database from '{VECTORDB_PERSIST_DIR}'...\")\n",
    "        vectordb = Chroma(\n",
    "            persist_directory=VECTORDB_PERSIST_DIR,\n",
    "            embedding_function=embedding_model # 现在这是 DashScopeEmbeddings\n",
    "        )\n",
    "        print(\"✅ Existing vector database loaded successfully.\")\n",
    "    else:\n",
    "        print(f\"    No existing database found. Creating new vector database at '{VECTORDB_PERSIST_DIR}'...\")\n",
    "        print(\"    (This may take a few minutes, calling Chroma.from_documents()...)\")\n",
    "        vectordb = Chroma.from_documents(\n",
    "            documents=documents, # 清理过的列表\n",
    "            embedding=embedding_model, # 现在这是 DashScopeEmbeddings\n",
    "            persist_directory=VECTORDB_PERSIST_DIR\n",
    "        )\n",
    "        print(f\"✅ New vector database created and persisted.\")\n",
    "\n",
    "    retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
    "    print(\"✅ Retriever created successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR: Failed during vector database creation/loading: {e}\")\n",
    "    print(\"    This error is now coming from the native DashScopeEmbeddings class.\")\n",
    "    exit(1)"
   ],
   "id": "d899624110fd391d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loading existing vector database from './data/database/ttp_chroma_db_qwen_compat/'...\n",
      "✅ Existing vector database loaded successfully.\n",
      "✅ Retriever created successfully.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:02:33.864229Z",
     "start_time": "2025-10-27T18:02:33.768333Z"
    }
   },
   "cell_type": "code",
   "source": "len(vectordb.get()['documents'])",
   "id": "466cf4ec335d8e87",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "508"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:02:33.892989Z",
     "start_time": "2025-10-27T18:02:33.882654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "prompt_template_str = \"\"\"\n",
    "You are a top-tier cybersecurity CTI analyst and an expert in the MITRE ATT&CK framework.\n",
    "Your task is to establish a clear, logical connection between a CTI description and its corresponding ATT&CK technique (TTP).\n",
    "\n",
    "Please follow these steps strictly:\n",
    "1.  Analyze the provided **[CTI Input]**, identifying the key actions, tools, or targets (e.g., \"used macros\", \"download and deploy\", \"SOCKS proxy\").\n",
    "2.  Review the **[Retrieved TTP Context]** to understand the official definitions of the retrieved techniques.\n",
    "3.  Generate a detailed **[Reasoning Process]** that explains exactly why the key actions in the **[CTI Input]** match the definition of the **[Target TTP]**. You must specify which words or phrases from the input correspond to which aspects of the technique's definition.\n",
    "4.  Your response must **only** contain the detailed **[Reasoning Process]**.\n",
    "\n",
    "---\n",
    "[CTI Input]:\n",
    "{input_cti}\n",
    "\n",
    "[Target TTP]:\n",
    "{target_ttp}\n",
    "\n",
    "[Retrieved TTP Context]:\n",
    "{context}\n",
    "---\n",
    "\n",
    "Please output only your **[Reasoning Process]**:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template_str)"
   ],
   "id": "6668849e4200ed46",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:02:33.903534Z",
     "start_time": "2025-10-27T18:02:33.899418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# --- 4.2 辅助函数 ---\n",
    "def format_docs(docs):\n",
    "    \"\"\"Formats retrieved documents for the prompt.\"\"\"\n",
    "    formatted = []\n",
    "    for doc in docs:\n",
    "        metadata = doc.metadata\n",
    "        formatted.append(f\"ID: {metadata.get('id', 'N/A')}\\nName: {metadata.get('name', 'N/A')}\\nDescription: {doc.page_content}\")\n",
    "    return \"\\n---\\n\".join(formatted)"
   ],
   "id": "68f463bb7a8b4390",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:02:33.913711Z",
     "start_time": "2025-10-27T18:02:33.910294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # --- 4.3 构建链 (LCEL) ---\n",
    "# retriever_chain = (\n",
    "#     RunnableLambda(lambda x: x['input'])\n",
    "#     | retriever\n",
    "#     | format_docs\n",
    "# )\n",
    "#\n",
    "# generation_chain = (\n",
    "#     {\n",
    "#         \"context\": retriever_chain,\n",
    "#         \"original_input\": RunnablePassthrough()\n",
    "#     }\n",
    "#     | RunnableParallel(\n",
    "#         input_cti=lambda x: x['original_input']['input'],\n",
    "#         target_ttp=lambda x: x['original_input']['output'],\n",
    "#         context=lambda x: x['context']\n",
    "#       )\n",
    "#     | prompt\n",
    "#     | llm # (这仍然是 ChatOpenAI 包装器)\n",
    "#     | StrOutputParser()\n",
    "# )\n",
    "#\n",
    "# print(\"✅ RAG chain built successfully.\")\n",
    "\n",
    "# 链 1: 只负责检索 (将在单线程循环中运行)\n",
    "retriever_chain = (\n",
    "    RunnableLambda(lambda x: x['input']) # 接收 {'input': ...}\n",
    "    | retriever\n",
    "    | format_docs # 返回上下文(context)字符串\n",
    ")\n",
    "print(\"✅ Retriever chain (for sequential retrieval) built successfully.\")\n",
    "\n",
    "# 链 2: 只负责生成 (将在并发批处理中运行)\n",
    "# 这条链需要一个字典，包含所有 Prompt 所需的信息\n",
    "llm_chain = (\n",
    "    prompt # (Prompt 模板保持不变)\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(\"✅ LLM chain (for batch generation) built successfully.\")"
   ],
   "id": "a5a46cd6c3344598",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Retriever chain (for sequential retrieval) built successfully.\n",
      "✅ LLM chain (for batch generation) built successfully.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:02:33.928014Z",
     "start_time": "2025-10-27T18:02:33.924554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 4.4 测试 RAG 链 ---\n",
    "# print(\"    Testing RAG chain with a sample input...\")\n",
    "# try:\n",
    "#     test_chain_input = {\n",
    "#         'input': \"TrickBot has used macros in Excel documents to download and deploy the malware on the user’s machine.\",\n",
    "#         'output': \"T1059: Command and Scripting Interpreter\"\n",
    "#     }\n",
    "#     test_explanation = generation_chain.invoke(test_chain_input)\n",
    "#     print(\"✅ RAG chain test successful.\")\n",
    "#     print(\"--- Sample Explanation (from chain test) ---\")\n",
    "#     print(test_explanation)\n",
    "#     print(\"------------------------------------------\")\n",
    "# except Exception as e:\n",
    "#     print(f\"⚠️ WARNING: RAG chain test failed: {e}\")"
   ],
   "id": "c6f4baf820c2b313",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:44:44.401221Z",
     "start_time": "2025-10-27T18:02:33.934872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# try:\n",
    "#     with open(DATASET1_PATH, 'r', encoding='utf-8') as f:\n",
    "#         dataset1 = json.load(f)\n",
    "#     print(f\"✅ Dataset 1 loaded successfully with {len(dataset1)} records.\")\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"❌ ERROR: Dataset 1 file not found at '{DATASET1_PATH}'\")\n",
    "#     exit(1)\n",
    "# except json.JSONDecodeError:\n",
    "#     print(f\"❌ ERROR: Dataset 1 file '{DATASET1_PATH}' is not a valid JSON file.\")\n",
    "#     exit(1)\n",
    "# except Exception as e:\n",
    "#     print(f\"❌ An unexpected error occurred while loading Dataset 1: {e}\")\n",
    "#     exit(1)\n",
    "\n",
    "try:\n",
    "    with open(DATASET1_PATH, 'r', encoding='utf-8') as f:\n",
    "        dataset1 = json.load(f)\n",
    "    print(f\"✅ Dataset 1 loaded successfully with {len(dataset1)} records.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ ERROR: Dataset 1 file not found at '{DATASET1_PATH}'\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"❌ An unexpected error occurred while loading Dataset 1: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# 这是“阶段 B” (LLM) 所需的输入列表\n",
    "inputs_for_llm_batch = []\n",
    "valid_items_for_zip = [] # 仍然需要这个来在最后拼装\n",
    "\n",
    "print(f\"--- Starting Phase A: Retrieving context for {len(dataset1)} items (Single-threaded)... ---\")\n",
    "# 这个循环非常快 (几分钟)，因为它只是在查询本地数据库\n",
    "for item in tqdm(dataset1, desc=\"Phase A: Retrieving Context\"):\n",
    "    if not isinstance(item, dict) or 'input' not in item or 'output' not in item:\n",
    "        print(f\"⚠️ WARNING: Skipping malformed data item: {item}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # --- !! 关键修复：我们在这里手动执行 retriever_chain !! ---\n",
    "\n",
    "        # 1. 准备纯字符串查询\n",
    "        query_string = item['input']\n",
    "        if not query_string: # 额外的安全检查\n",
    "             print(f\"⚠️ WARNING: Skipping item with empty 'input' string.\")\n",
    "             continue\n",
    "\n",
    "        # 2. (手动) 调用 retriever (这会调用 DashScopeEmbeddings)\n",
    "        #    retriever 是在 [Step 3] 中创建的\n",
    "        retrieved_docs = retriever.invoke(query_string)\n",
    "\n",
    "        # 3. (手动) 调用 format_docs (这是在 [Step 4.2] 中定义的)\n",
    "        retrieved_context_str = format_docs(retrieved_docs)\n",
    "\n",
    "        # --- 修复结束 ---\n",
    "\n",
    "        # 4. 准备 LLM 链的输入\n",
    "        llm_input_dict = {\n",
    "            \"input_cti\": item['input'],\n",
    "            \"target_ttp\": item['output'],\n",
    "            \"context\": retrieved_context_str\n",
    "        }\n",
    "\n",
    "        # 5. 将准备好的数据添加到列表中\n",
    "        inputs_for_llm_batch.append(llm_input_dict)\n",
    "        valid_items_for_zip.append(item) # 保持 item 同步\n",
    "\n",
    "    except Exception as e:\n",
    "        # 捕获 `retriever.invoke` 或 `format_docs` 中的任何错误\n",
    "        print(f\"❌ ERROR during retrieval for item '{item.get('input', 'N/A')[:50]}...': {e}\")\n",
    "        # (我们跳过这个失败的条目)\n",
    "\n",
    "print(f\"✅ Phase A complete. Prepared {len(inputs_for_llm_batch)} items for LLM generation.\")-"
   ],
   "id": "47d4a092224bc1b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset 1 loaded successfully with 4000 records.\n",
      "--- Starting Phase A: Retrieving context for 4000 items (Single-threaded)... ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Phase A: Retrieving Context: 100%|██████████| 4000/4000 [42:10<00:00,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Phase A complete. Prepared 4000 items for LLM generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:44:44.548031Z",
     "start_time": "2025-10-27T18:44:44.544901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import time\n",
    "# new_dataset_with_explanation = []\n",
    "# error_count = 0\n",
    "# max_retries = 3\n",
    "#\n",
    "# # --- 关键修改 1: 我们使用 enumerate() 来获取索引 (i) ---\n",
    "# for i, item in enumerate(tqdm(dataset1, desc=\"Generating Explanations\")):\n",
    "#     if not isinstance(item, dict) or 'input' not in item or 'output' not in item:\n",
    "#         # (使用 tqdm.write 打印警告)\n",
    "#         tqdm.write(f\"⚠️ WARNING: Skipping malformed data item: {item}\")\n",
    "#         continue\n",
    "#\n",
    "#     chain_input = {\"input\": item['input'], \"output\": item['output']}\n",
    "#\n",
    "#     explanation = None\n",
    "#     retries = 0\n",
    "#     while retries < max_retries:\n",
    "#         try:\n",
    "#             explanation = generation_chain.invoke(chain_input)\n",
    "#             break\n",
    "#         except Exception as e:\n",
    "#             retries += 1\n",
    "#             # (使用 tqdm.write 打印错误)\n",
    "#             tqdm.write(f\"⚠️ WARNING: Error processing '{item.get('input', 'N/A')[:50]}...' (Attempt {retries}/{max_retries}): {e}\")\n",
    "#             if retries < max_retries:\n",
    "#                 tqdm.write(\"    Retrying after 5 seconds...\")\n",
    "#                 time.sleep(5)\n",
    "#             else:\n",
    "#                 tqdm.write(f\"❌ ERROR: Max retries reached. Failed to generate explanation for this item.\")\n",
    "#                 error_count += 1\n",
    "#\n",
    "#     if explanation:\n",
    "#         final_output_string = f\"[Reasoning Process]:\\n{explanation.strip()}\\n\\n[Final Answer]:\\n{item['output']}\"\n",
    "#\n",
    "#         # --- 关键修改 2: 在这里添加实时打印 ---\n",
    "#         tqdm.write(\"\\n\" + \"=\"*80)\n",
    "#         tqdm.write(f\"--- [Item {i+1}/{len(dataset1)}] ---\")\n",
    "#         tqdm.write(f\"Input:    {item['input'][:150]}...\") # 打印Input（截断）\n",
    "#         tqdm.write(f\"Target:   {item['output']}\")\n",
    "#         tqdm.write(f\"--- Generated Explanation (Real-time) ---\")\n",
    "#         tqdm.write(explanation.strip()) # 打印RAG模型的实时输出\n",
    "#         tqdm.write(\"=\"*80 + \"\\n\")\n",
    "#         # --- 实时打印结束 ---\n",
    "#\n",
    "#         new_sample = {\n",
    "#             \"instruction\": item.get('instruction', \"Find the techniques and ID from MITRE ATT&CK framework.\"),\n",
    "#             \"input\": item['input'],\n",
    "#             \"output\": final_output_string\n",
    "#         }\n",
    "#         new_dataset_with_explanation.append(new_sample)"
   ],
   "id": "d7746c5c30b6ee4c",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:23:57.671434Z",
     "start_time": "2025-10-27T18:44:44.568095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 辅助函数：创建“小批次” ---\n",
    "def create_mini_batches(list_data, batch_size):\n",
    "    for i in range(0, len(list_data), batch_size):\n",
    "        yield list_data[i:i + batch_size]\n",
    "\n",
    "# --- 设置并发数 ---\n",
    "# (我们现在可以安全地使用高并发，因为它只调用 Qwen API)\n",
    "MAX_CONCURRENCY = 30  # (从 10 增加到 30，以接近 22 分钟的总时间)\n",
    "MINI_BATCH_SIZE = 30  # (TQDM 进度条每 30 个条目更新一次)\n",
    "\n",
    "new_dataset_with_explanation = []\n",
    "error_count = 0\n",
    "\n",
    "# 为 LLM 输入和原始 item 创建小批次\n",
    "llm_input_batches = list(create_mini_batches(inputs_for_llm_batch, MINI_BATCH_SIZE))\n",
    "item_batches = list(create_mini_batches(valid_items_for_zip, MINI_BATCH_SIZE))\n",
    "\n",
    "print(f\"    Total items: {len(inputs_for_llm_batch)}\")\n",
    "print(f\"    Mini-batch size (per progress bar update): {MINI_BATCH_SIZE}\")\n",
    "print(f\"    Number of mini-batches (total progress bar steps): {len(llm_input_batches)}\")\n",
    "print(f\"    Internal concurrency (API calls): {MAX_CONCURRENCY}\")\n",
    "print(f\"    Estimated time per step: ~10-15 seconds.\")\n",
    "print(f\"    Estimated total time: ~{ (len(llm_input_batches) * 12) / 60 :.0f} minutes.\")\n",
    "\n",
    "# --- 我们在自己的循环中调用 .batch()，并用 tqdm 包装它 ---\n",
    "try:\n",
    "    for i in tqdm(range(len(llm_input_batches)), desc=\"Phase B: Generating Explanations\"):\n",
    "        llm_input_batch = llm_input_batches[i] # 这是包含 {input_cti, ...} 的字典列表\n",
    "        original_item_batch = item_batches[i]\n",
    "\n",
    "        try:\n",
    "            # 在“小批次”上调用 llm_chain.batch()\n",
    "            # (这不再调用 ChromaDB，只调用 Qwen)\n",
    "            results = llm_chain.batch(\n",
    "                llm_input_batch, # (注意：我们现在传递的是 llm_input_batch)\n",
    "                config={\"max_concurrency\": MAX_CONCURRENCY}\n",
    "            )\n",
    "\n",
    "            # 处理这个小批次的结果\n",
    "            for item, explanation in zip(original_item_batch, results):\n",
    "                if isinstance(explanation, Exception):\n",
    "                    tqdm.write(f\"\\n⚠️ WARNING (in batch {i}): Error processing '{item['input'][:50]}...': {explanation}\\n\")\n",
    "                    error_count += 1\n",
    "                else:\n",
    "                    final_output_string = f\"[Reasoning Process]:\\n{explanation.strip()}\\n\\n[Final Answer]:\\n{item['output']}\"\n",
    "                    new_sample = {\n",
    "                        \"instruction\": item.get('instruction', \"Find the techniques and ID from MITRE ATT&CK framework.\"),\n",
    "                        \"input\": item['input'],\n",
    "                        \"output\": final_output_string\n",
    "                    }\n",
    "                    new_dataset_with_explanation.append(new_sample)\n",
    "\n",
    "        except Exception as batch_error:\n",
    "            tqdm.write(f\"\\n❌ FATAL ERROR during mini-batch {i} (Qwen API Error?): {batch_error}\")\n",
    "            tqdm.write(\"    Skipping this entire batch...\")\n",
    "            error_count += len(llm_input_batch)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\n--- User interrupted. Stopping batch processing... ---\")\n",
    "    pass\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ ERROR: An unexpected fatal error occurred: {e}\")\n",
    "    exit(1)"
   ],
   "id": "b5dd6c11024e70b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total items: 4000\n",
      "    Mini-batch size (per progress bar update): 30\n",
      "    Number of mini-batches (total progress bar steps): 134\n",
      "    Internal concurrency (API calls): 30\n",
      "    Estimated time per step: ~10-15 seconds.\n",
      "    Estimated total time: ~27 minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Phase B: Generating Explanations: 100%|██████████| 134/134 [39:13<00:00, 17.56s/it]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:23:57.789338Z",
     "start_time": "2025-10-27T19:23:57.691503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"\\n--- [Step 7] Generation Complete ---\")\n",
    "print(f\"    Successfully generated explanations: {len(new_dataset_with_explanation)} records\")\n",
    "print(f\"    Failed items: {error_count} records\")\n",
    "\n",
    "print(f\"\\n--- [Step 8] Saving new dataset to '{OUTPUT_DATASET_PATH}'... ---\")\n",
    "try:\n",
    "    with open(OUTPUT_DATASET_PATH, 'w', encoding='utf-8') as f:\n",
    "        json.dump(new_dataset_with_explanation, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"✅ New dataset saved successfully!\")\n",
    "    if new_dataset_with_explanation:\n",
    "      print(\"\\n--- Example of new dataset (first item) ---\")\n",
    "      print(json.dumps(new_dataset_with_explanation[0], indent=2, ensure_ascii=False))\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR: Failed to save the new dataset: {e}\")\n",
    "\n",
    "print(\"\\n--- Script Finished ---\")"
   ],
   "id": "88cc30cc571cb5c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [Step 7] Generation Complete ---\n",
      "    Successfully generated explanations: 4000 records\n",
      "    Failed items: 0 records\n",
      "\n",
      "--- [Step 8] Saving new dataset to './data/output/sft_dataset_with_qwen_compat_explanations.json'... ---\n",
      "✅ New dataset saved successfully!\n",
      "\n",
      "--- Example of new dataset (first item) ---\n",
      "{\n",
      "  \"instruction\": \"Find the techniques and ID from MITRE ATT&CK framework.\",\n",
      "  \"input\": \"TrickBot has used macros in Excel documents to download and deploy the malware on the user’s machine.\",\n",
      "  \"output\": \"[Reasoning Process]:\\n[Reasoning Process]: The CTI input states that \\\"TrickBot has used macros in Excel documents to download and deploy the malware on the user’s machine.\\\" The key action here is the use of **macros**—specifically within an **Excel document**—to execute malicious behavior, namely downloading and deploying malware. Macros in Microsoft Office applications, including Excel, are implemented using **Visual Basic for Applications (VBA)**, which is a scripting language that allows automation and execution of commands within the Office environment. The execution of VBA macros falls under the broader category of **scripting interpreters**, as these scripts are interpreted and executed by the host application at runtime.\\n\\nThe target TTP, **T1059: Command and Scripting Interpreter**, refers to techniques where adversaries leverage built-in command-line or scripting environments (such as PowerShell, Windows Command Shell, or **VBA**) to execute malicious commands. According to MITRE, this includes abuse of **Windows Script Host, PowerShell, Unix shell, and other script interpreters**, and explicitly encompasses **Office-based scripting via VBA macros** when used to run malicious code. Although the retrieved context does not directly quote T1059's definition, it is well established in the ATT&CK framework that **T1059.003 - Windows Command Shell**, **T1059.005 - Visual Basic**, and related sub-techniques cover VBA macro execution in Office documents.\\n\\nIn this case, the phrase **\\\"used macros in Excel documents\\\"** directly corresponds to the use of **VBA as a scripting interpreter** embedded within a legitimate application (Excel), which aligns with the core definition of T1059. Furthermore, the purpose of these macros—to **download and deploy malware**—indicates active command execution, such as calling external utilities (e.g., `certutil.exe`, `powershell.exe`) or using HTTP requests via scripts, all of which are typical behaviors under T1059.\\n\\nWhile the retrieved TTP contexts include **T1137.001 (Office Template Macros)** and **T1221 (Template Injection)**, both of which involve persistence mechanisms through template manipulation, they focus on **persistence via startup templates** or **remote template loading**, rather than the general execution of malicious scripts. Similarly, **T1564.007 (VBA Stomping)** relates to artifact hiding, not the act of script execution itself. None of these better match the described activity than T1059, which directly covers the **execution of malicious VBA macros as a scripting interpreter**.\\n\\nTherefore, the use of Excel macros to run code that downloads and deploys malware constitutes a clear instance of **abusing a built-in scripting interpreter (VBA)**, making **T1059: Command and Scripting Interpreter** the most accurate and logically supported ATT&CK technique based on the CTI input.\\n\\n[Final Answer]:\\nT1059: Command and Scripting Interpreter\"\n",
      "}\n",
      "\n",
      "--- Script Finished ---\n"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
