{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-27T18:02:32.563324Z",
     "start_time": "2025-10-27T18:02:27.198106Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:02:32.572025Z",
     "start_time": "2025-10-27T18:02:32.566328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.embeddings import DashScopeEmbeddings"
   ],
   "id": "120591c5727a4152",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:02:32.679690Z",
     "start_time": "2025-10-27T18:02:32.674856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env_path = Path('.') / '.env'\n",
    "load_dotenv(dotenv_path=env_path)\n"
   ],
   "id": "dd10741805d25e30",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:02:33.498105Z",
     "start_time": "2025-10-27T18:02:32.690452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "QWEN_API_KEY = os.environ.get(\"DASHSCOPE_API_KEY\")\n",
    "QWEN_BASE_URL = os.environ.get(\"DASHSCOPE_API_URL\")\n",
    "llm = ChatOpenAI(\n",
    "        model=\"qwen-plus-latest\",\n",
    "        temperature=0.3,\n",
    "        api_key=QWEN_API_KEY,\n",
    "        base_url=QWEN_BASE_URL\n",
    "    )\n",
    "\n",
    "embedding_model = DashScopeEmbeddings(\n",
    "        model=os.environ.get(\"EMBEDDING_MODEL\")\n",
    "    )"
   ],
   "id": "910b9f41ed39d8b4",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:02:33.506480Z",
     "start_time": "2025-10-27T18:02:33.503704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "DATASET1_PATH = \"./data/input/sft_dataset_4000.json\"\n",
    "DATASET2_PATH = \"./data/input/MITRE-ATTACK_dataset_test.json\"\n",
    "OUTPUT_DATASET_PATH = \"./data/output/sft_dataset_with_qwen_compat_explanations.json\"\n",
    "\n",
    "VECTORDB_PERSIST_DIR = \"./data/database/ttp_chroma_db_qwen_compat/\""
   ],
   "id": "d9f1555445519963",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:02:33.567341Z",
     "start_time": "2025-10-27T18:02:33.549090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.documents import Document\n",
    "try:\n",
    "    df_ttp = pd.read_json(DATASET2_PATH, orient=\"records\")\n",
    "    required_columns = ['ID', 'name', 'description']\n",
    "    if not all(col in df_ttp.columns for col in required_columns):\n",
    "        raise ValueError(f\"Dataset 2 JSON file must contain the following keys: {required_columns}\")\n",
    "    documents = []\n",
    "    skipped_rows = 0\n",
    "    for index, row in df_ttp.iterrows():\n",
    "        description_content = row['description']\n",
    "        if pd.isna(description_content) or not isinstance(description_content, str) or description_content.strip() == \"\":\n",
    "            print(f\"WARNING: Skipping row {index} (ID: {row.get('ID', 'N/A')}) due to missing, empty, or non-string description.\")\n",
    "            skipped_rows += 1\n",
    "            continue\n",
    "        doc = Document(\n",
    "            page_content=description_content,\n",
    "            metadata={'id': row['ID'], 'name': row['name']}\n",
    "        )\n",
    "        documents.append(doc)\n",
    "\n",
    "    print(f\"Loaded and converted {len(documents)} TTP documents.\")\n",
    "    if skipped_rows > 0:\n",
    "        print(f\"Skipped {skipped_rows} rows due to invalid/missing descriptions.\")\n",
    "\n",
    "    if not documents:\n",
    "        raise ValueError(\"No valid documents were loaded. Please check your JSON file for 'description' fields.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Knowledge Base file not found at '{DATASET2_PATH}'\")\n",
    "    exit(1)\n",
    "except ValueError as ve:\n",
    "    print(f\"ERROR: {ve}\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while loading Dataset 2: {e}\")\n",
    "    exit(1)"
   ],
   "id": "309254bad8f7482f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded and converted 508 TTP documents.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:02:33.758710Z",
     "start_time": "2025-10-27T18:02:33.653571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    if os.path.exists(VECTORDB_PERSIST_DIR) and os.listdir(VECTORDB_PERSIST_DIR):\n",
    "        print(f\"Loading existing vector database from '{VECTORDB_PERSIST_DIR}'...\")\n",
    "        vectordb = Chroma(\n",
    "            persist_directory=VECTORDB_PERSIST_DIR,\n",
    "            embedding_function=embedding_model\n",
    "        )\n",
    "        print(\"Existing vector database loaded successfully.\")\n",
    "    else:\n",
    "        print(f\"No existing database found. Creating new vector database at '{VECTORDB_PERSIST_DIR}'...\")\n",
    "        vectordb = Chroma.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=embedding_model,\n",
    "            persist_directory=VECTORDB_PERSIST_DIR\n",
    "        )\n",
    "    retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed during vector database creation/loading: {e}\")\n",
    "    print(\"This error is now coming from the native DashScopeEmbeddings class.\")\n",
    "    exit(1)"
   ],
   "id": "d899624110fd391d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loading existing vector database from './data/database/ttp_chroma_db_qwen_compat/'...\n",
      "✅ Existing vector database loaded successfully.\n",
      "✅ Retriever created successfully.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:02:33.864229Z",
     "start_time": "2025-10-27T18:02:33.768333Z"
    }
   },
   "cell_type": "code",
   "source": "len(vectordb.get()['documents'])",
   "id": "466cf4ec335d8e87",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "508"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:02:33.892989Z",
     "start_time": "2025-10-27T18:02:33.882654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "prompt_template_str = \"\"\"\n",
    "You are a top-tier cybersecurity CTI analyst and an expert in the MITRE ATT&CK framework.\n",
    "Your task is to establish a clear, logical connection between a CTI description and its corresponding ATT&CK technique (TTP).\n",
    "\n",
    "Please follow these steps strictly:\n",
    "1.  Analyze the provided **[CTI Input]**, identifying the key actions, tools, or targets (e.g., \"used macros\", \"download and deploy\", \"SOCKS proxy\").\n",
    "2.  Review the **[Retrieved TTP Context]** to understand the official definitions of the retrieved techniques.\n",
    "3.  Generate a detailed **[Reasoning Process]** that explains exactly why the key actions in the **[CTI Input]** match the definition of the **[Target TTP]**. You must specify which words or phrases from the input correspond to which aspects of the technique's definition.\n",
    "4.  Your response must **only** contain the detailed **[Reasoning Process]**.\n",
    "\n",
    "---\n",
    "[CTI Input]:\n",
    "{input_cti}\n",
    "\n",
    "[Target TTP]:\n",
    "{target_ttp}\n",
    "\n",
    "[Retrieved TTP Context]:\n",
    "{context}\n",
    "---\n",
    "\n",
    "Please output only your **[Reasoning Process]**:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template_str)"
   ],
   "id": "6668849e4200ed46",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:02:33.903534Z",
     "start_time": "2025-10-27T18:02:33.899418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "def format_docs(docs):\n",
    "    \"\"\"Formats retrieved documents for the prompt.\"\"\"\n",
    "    formatted = []\n",
    "    for doc in docs:\n",
    "        metadata = doc.metadata\n",
    "        formatted.append(f\"ID: {metadata.get('id', 'N/A')}\\nName: {metadata.get('name', 'N/A')}\\nDescription: {doc.page_content}\")\n",
    "    return \"\\n---\\n\".join(formatted)"
   ],
   "id": "68f463bb7a8b4390",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:02:33.913711Z",
     "start_time": "2025-10-27T18:02:33.910294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 链 1: 只负责检索 (将在单线程循环中运行)\n",
    "retriever_chain = (\n",
    "    RunnableLambda(lambda x: x['input'])\n",
    "    | retriever\n",
    "    | format_docs\n",
    ")\n",
    "\n",
    "# 链 2: 只负责生成 (将在并发批处理中运行)\n",
    "# 这条链需要一个字典，包含所有 Prompt 所需的信息\n",
    "llm_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ],
   "id": "a5a46cd6c3344598",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Retriever chain (for sequential retrieval) built successfully.\n",
      "✅ LLM chain (for batch generation) built successfully.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:02:33.928014Z",
     "start_time": "2025-10-27T18:02:33.924554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---测试 RAG 链 ---\n",
    "# print(\"    Testing RAG chain with a sample input...\")\n",
    "# try:\n",
    "#     test_chain_input = {\n",
    "#         'input': \"TrickBot has used macros in Excel documents to download and deploy the malware on the user’s machine.\",\n",
    "#         'output': \"T1059: Command and Scripting Interpreter\"\n",
    "#     }\n",
    "#     test_explanation = generation_chain.invoke(test_chain_input)\n",
    "#     print(\" RAG chain test successful.\")\n",
    "#     print(\"--- Sample Explanation (from chain test) ---\")\n",
    "#     print(test_explanation)\n",
    "#     print(\"------------------------------------------\")\n",
    "# except Exception as e:\n",
    "#     print(f\" WARNING: RAG chain test failed: {e}\")"
   ],
   "id": "c6f4baf820c2b313",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:44:44.401221Z",
     "start_time": "2025-10-27T18:02:33.934872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "try:\n",
    "    with open(DATASET1_PATH, 'r', encoding='utf-8') as f:\n",
    "        dataset1 = json.load(f)\n",
    "    print(f\"Dataset 1 loaded successfully with {len(dataset1)} records.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Dataset 1 file not found at '{DATASET1_PATH}'\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while loading Dataset 1: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "inputs_for_llm_batch = []\n",
    "valid_items_for_zip = []\n",
    "\n",
    "print(f\"--- Starting Phase A: Retrieving context for {len(dataset1)} items (Single-threaded)... ---\")\n",
    "for item in tqdm(dataset1, desc=\"Phase A: Retrieving Context\"):\n",
    "    if not isinstance(item, dict) or 'input' not in item or 'output' not in item:\n",
    "        print(f\" WARNING: Skipping malformed data item: {item}\")\n",
    "        continue\n",
    "    try:\n",
    "        query_string = item['input']\n",
    "        if not query_string: # 额外的安全检查\n",
    "             print(f\"WARNING: Skipping item with empty 'input' string.\")\n",
    "             continue\n",
    "\n",
    "        retrieved_docs = retriever.invoke(query_string)\n",
    "        retrieved_context_str = format_docs(retrieved_docs)\n",
    "        llm_input_dict = {\n",
    "            \"input_cti\": item['input'],\n",
    "            \"target_ttp\": item['output'],\n",
    "            \"context\": retrieved_context_str\n",
    "        }\n",
    "\n",
    "        inputs_for_llm_batch.append(llm_input_dict)\n",
    "        valid_items_for_zip.append(item)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR during retrieval for item '{item.get('input', 'N/A')[:50]}...': {e}\")\n"
   ],
   "id": "47d4a092224bc1b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset 1 loaded successfully with 4000 records.\n",
      "--- Starting Phase A: Retrieving context for 4000 items (Single-threaded)... ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Phase A: Retrieving Context: 100%|██████████| 4000/4000 [42:10<00:00,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Phase A complete. Prepared 4000 items for LLM generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:44:44.548031Z",
     "start_time": "2025-10-27T18:44:44.544901Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d7746c5c30b6ee4c",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:23:57.671434Z",
     "start_time": "2025-10-27T18:44:44.568095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def create_mini_batches(list_data, batch_size):\n",
    "    for i in range(0, len(list_data), batch_size):\n",
    "        yield list_data[i:i + batch_size]\n",
    "\n",
    "# 设置并发数\n",
    "MAX_CONCURRENCY = 30\n",
    "MINI_BATCH_SIZE = 30\n",
    "\n",
    "new_dataset_with_explanation = []\n",
    "error_count = 0\n",
    "\n",
    "\n",
    "llm_input_batches = list(create_mini_batches(inputs_for_llm_batch, MINI_BATCH_SIZE))\n",
    "item_batches = list(create_mini_batches(valid_items_for_zip, MINI_BATCH_SIZE))\n",
    "\n",
    "print(f\"    Total items: {len(inputs_for_llm_batch)}\")\n",
    "print(f\"    Mini-batch size (per progress bar update): {MINI_BATCH_SIZE}\")\n",
    "print(f\"    Number of mini-batches (total progress bar steps): {len(llm_input_batches)}\")\n",
    "print(f\"    Internal concurrency (API calls): {MAX_CONCURRENCY}\")\n",
    "print(f\"    Estimated time per step: ~10-15 seconds.\")\n",
    "print(f\"    Estimated total time: ~{ (len(llm_input_batches) * 12) / 60 :.0f} minutes.\")\n",
    "\n",
    "\n",
    "try:\n",
    "    for i in tqdm(range(len(llm_input_batches)), desc=\"Phase B: Generating Explanations\"):\n",
    "        llm_input_batch = llm_input_batches[i]\n",
    "        original_item_batch = item_batches[i]\n",
    "\n",
    "        try:\n",
    "            results = llm_chain.batch(\n",
    "                llm_input_batch,\n",
    "                config={\"max_concurrency\": MAX_CONCURRENCY}\n",
    "            )\n",
    "            for item, explanation in zip(original_item_batch, results):\n",
    "                if isinstance(explanation, Exception):\n",
    "                    tqdm.write(f\"\\nWARNING (in batch {i}): Error processing '{item['input'][:50]}...': {explanation}\\n\")\n",
    "                    error_count += 1\n",
    "                else:\n",
    "                    final_output_string = f\"[Reasoning Process]:\\n{explanation.strip()}\\n\\n[Final Answer]:\\n{item['output']}\"\n",
    "                    new_sample = {\n",
    "                        \"instruction\": item.get('instruction', \"Find the techniques and ID from MITRE ATT&CK framework.\"),\n",
    "                        \"input\": item['input'],\n",
    "                        \"output\": final_output_string\n",
    "                    }\n",
    "                    new_dataset_with_explanation.append(new_sample)\n",
    "\n",
    "        except Exception as batch_error:\n",
    "            tqdm.write(f\"\\n FATAL ERROR during mini-batch {i} (Qwen API Error?): {batch_error}\")\n",
    "            tqdm.write(\"    Skipping this entire batch...\")\n",
    "            error_count += len(llm_input_batch)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\n--- User interrupted. Stopping batch processing... ---\")\n",
    "    pass\n",
    "except Exception as e:\n",
    "    print(f\"\\n ERROR: An unexpected fatal error occurred: {e}\")\n",
    "    exit(1)"
   ],
   "id": "b5dd6c11024e70b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total items: 4000\n",
      "    Mini-batch size (per progress bar update): 30\n",
      "    Number of mini-batches (total progress bar steps): 134\n",
      "    Internal concurrency (API calls): 30\n",
      "    Estimated time per step: ~10-15 seconds.\n",
      "    Estimated total time: ~27 minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Phase B: Generating Explanations: 100%|██████████| 134/134 [39:13<00:00, 17.56s/it]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T19:23:57.789338Z",
     "start_time": "2025-10-27T19:23:57.691503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"\\n--- [Step 7] Generation Complete ---\")\n",
    "print(f\"    Successfully generated explanations: {len(new_dataset_with_explanation)} records\")\n",
    "print(f\"    Failed items: {error_count} records\")\n",
    "\n",
    "print(f\"\\n--- [Step 8] Saving new dataset to '{OUTPUT_DATASET_PATH}'... ---\")\n",
    "try:\n",
    "    with open(OUTPUT_DATASET_PATH, 'w', encoding='utf-8') as f:\n",
    "        json.dump(new_dataset_with_explanation, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"New dataset saved successfully!\")\n",
    "    if new_dataset_with_explanation:\n",
    "      print(\"\\n--- Example of new dataset (first item) ---\")\n",
    "      print(json.dumps(new_dataset_with_explanation[0], indent=2, ensure_ascii=False))\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to save the new dataset: {e}\")\n",
    "\n",
    "print(\"\\n--- Script Finished ---\")"
   ],
   "id": "88cc30cc571cb5c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [Step 7] Generation Complete ---\n",
      "    Successfully generated explanations: 4000 records\n",
      "    Failed items: 0 records\n",
      "\n",
      "--- [Step 8] Saving new dataset to './data/output/sft_dataset_with_qwen_compat_explanations.json'... ---\n",
      "✅ New dataset saved successfully!\n",
      "\n",
      "--- Example of new dataset (first item) ---\n",
      "{\n",
      "  \"instruction\": \"Find the techniques and ID from MITRE ATT&CK framework.\",\n",
      "  \"input\": \"TrickBot has used macros in Excel documents to download and deploy the malware on the user’s machine.\",\n",
      "  \"output\": \"[Reasoning Process]:\\n[Reasoning Process]: The CTI input states that \\\"TrickBot has used macros in Excel documents to download and deploy the malware on the user’s machine.\\\" The key action here is the use of **macros**—specifically within an **Excel document**—to execute malicious behavior, namely downloading and deploying malware. Macros in Microsoft Office applications, including Excel, are implemented using **Visual Basic for Applications (VBA)**, which is a scripting language that allows automation and execution of commands within the Office environment. The execution of VBA macros falls under the broader category of **scripting interpreters**, as these scripts are interpreted and executed by the host application at runtime.\\n\\nThe target TTP, **T1059: Command and Scripting Interpreter**, refers to techniques where adversaries leverage built-in command-line or scripting environments (such as PowerShell, Windows Command Shell, or **VBA**) to execute malicious commands. According to MITRE, this includes abuse of **Windows Script Host, PowerShell, Unix shell, and other script interpreters**, and explicitly encompasses **Office-based scripting via VBA macros** when used to run malicious code. Although the retrieved context does not directly quote T1059's definition, it is well established in the ATT&CK framework that **T1059.003 - Windows Command Shell**, **T1059.005 - Visual Basic**, and related sub-techniques cover VBA macro execution in Office documents.\\n\\nIn this case, the phrase **\\\"used macros in Excel documents\\\"** directly corresponds to the use of **VBA as a scripting interpreter** embedded within a legitimate application (Excel), which aligns with the core definition of T1059. Furthermore, the purpose of these macros—to **download and deploy malware**—indicates active command execution, such as calling external utilities (e.g., `certutil.exe`, `powershell.exe`) or using HTTP requests via scripts, all of which are typical behaviors under T1059.\\n\\nWhile the retrieved TTP contexts include **T1137.001 (Office Template Macros)** and **T1221 (Template Injection)**, both of which involve persistence mechanisms through template manipulation, they focus on **persistence via startup templates** or **remote template loading**, rather than the general execution of malicious scripts. Similarly, **T1564.007 (VBA Stomping)** relates to artifact hiding, not the act of script execution itself. None of these better match the described activity than T1059, which directly covers the **execution of malicious VBA macros as a scripting interpreter**.\\n\\nTherefore, the use of Excel macros to run code that downloads and deploys malware constitutes a clear instance of **abusing a built-in scripting interpreter (VBA)**, making **T1059: Command and Scripting Interpreter** the most accurate and logically supported ATT&CK technique based on the CTI input.\\n\\n[Final Answer]:\\nT1059: Command and Scripting Interpreter\"\n",
      "}\n",
      "\n",
      "--- Script Finished ---\n"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
